### Рекурентные нейронные сети

Последовательности - текст (речь), временные ряды, звуковой сигнал (частотная модуляция)
- важно знать, что было ДО

NLP - Natural Language Processing

**Текстовая единица** = токен - символ, два символа(биграмма), три символа(триграмма), слово

Способ закодировать текст: Bag of Words (Мешок слов)

**Корпус документов** - генеральная выборка, все тексты (например, тексты отзывов)
**Документ** - смысловой законченный текст (1 отзыв)

Мошок слов представляет собой таблицу, где в колонках будут перечислены часто встречающиеся слова, которые есть в корпусе документов, в строках - будет перечень названий документов. Если слово встречается в этом документе, то на пересечении ставится 1, если нет - 0. 

Из минусов: стоп-слова (предлоги, союзы) - они часто встречаются в тексте, но не несут никакую смысловую нагрузку (библиотека NLTK содержит список стоп-слов разных языков)

**Обработка текста**:
- выкинуть стоп-слова
- привести слова в нижний регистр
- привести слова в начальную форму (лематизация) или оставить основу слова, без окончания (стемизация)
- кодирование текста: Мешок слов, TF (частота встречаемости слов в документе), IDF (обратная частота встречаемости слова в документах)

![Сверточная_нейросеть_для_текста](\image\image.png) - в качестве свертки используется слой Conv1D
- хорошо справляется с распознаванием тональности 

Рекурентная нейросеть:
- имеет в себе *скрытый* Hidden state (вектор, состояние)

![виды рекурентных нейросетей](\image\image-1.png)
* one-to-one - подаем и получаем одно значение (передаем изображение и получаем тип класса, к которому принадлежит изображение)
* one-to-many - подаем одно, получаем последовательность элементов (генерация текста)
* many-to-one - передаем большую последовательность, получаем одно значение (анализ тональности)
* many-to-many - передаем последовательность, получаем последовательность (машинный перевод)

![Общий вид рекурентных нейросетей](\image\image-2.png)

![Двунаправленная рекурентная нейросеть](\image\image-3.png) - состоит из двух рекурентных нейросетей: одна работает справа налево, другая слева направо. Информация о скрытых слоях в обоих направлениях объединяется, максимизируется или усредняется (polling) и передается на выход.

**Проблема рекурентных НС**: экспоненциальное снижение или повышение градиента -> модель не обучается (затухание градиента) или нестабильность процесса обучения (взрыв градиента)

**Решение** проблемы затухания градиента: юнит LSTM


**LSTM - Long Short To Memory** - юнит, запонимающий информация и удаляющий ее через какие-то промежутки времени

![Строение LSTM](\image\image-4.png)
- состоит из 4 маленьких нейронов с функцией активации (неизменные) и 3 ворот (gates):
* input-gate - ворота входа: 0 - пропускаем, 1 - нет
* этап обновления памяти 
* output-gate - ворота вывода

memory-state (cell-state) - хранит сохраненную информацию

Работа LSTM:
![forget-gate](\image\image-5.png): Информация из скрытых слоев h1 и h2 складывается (сложение векторов), пропускается через сигмоиду и выдается в виде вектора F (forget). Дальше вектор F поэлементно переумножается с вектором C (который несет в себе сохраненную информация) и пропускается дальше. При переумножении часть данных из С теряется (обнуляется).

![input-gate](\image\image-6.png): Объединеные вектора h1 и h2 проходят через сигмоиду и получается входной вектор I1. Также эти вектора проходят через функция активации тангенс и получается усредненный вектор I2. Оба вектора I1 и I2 поэлементно переумножаются и передается на cell-state. Там он складывается с вектором информации C - так происходит запоминание информации

 ![output-gate](\image\image-7.png): Объединеные вектора h1 и h2 проходят через сигмоиду и получается вектор O. В красной точке (на схеме) вектор С разходится на два пути: один идет по cell-state дальше, второй подается в output_gate. Она вектора остаются одинаковыми. В output-gate вектор С проходит через функцию гиперболического тангенса, поэлементно преумножается со входным вектором O и передается в скрытый слой для другого блока в виде вектора h1

Минусы LSTM:
* время обучения
* сложность обучения

**GRU - Gate Recurent Unit**

![GRU](\image\image-8.png)

- используется два gate (update и reset)
* update-gate - ворота обновления - определяют насколько важно сохранить предыдущее состояние скрытого слоя hidden-state. 0 - пропускаем (забываем), 1 - сохраняем

* reset-gate - принимает решение, как много информации, которая пришла к нам из прошлого юнита, должна быть забыта.

GRU обучает быстрее, но возможна незначительная потеря качества

![Сравнение моделей рекурентных нейросетей](\image\image-9.png)
