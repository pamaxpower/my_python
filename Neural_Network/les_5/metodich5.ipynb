{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DJiUvDz7EMdS"},"source":["# Введение в искусственные нейронные сети\n","# Урок 5. Рекуррентные нейронные сети"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i9cMG3PuEMdU"},"source":["## Содержание методического пособия:\n","\n","\n","<ol>\n","<li>Что такое Рекурретные нейронные сети</li>\n","<li>Архитектура Рекуррентных нейронных сетей</li>\n","<li>Пример на Keras рекуррентной нейронной сети</li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fYkFospmEMdV"},"source":["## Что такое Рекуррентные нейронные сети\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MsaaDJn2EMdW"},"source":["Нейронные сети которые мы разбирали ранее относяться к классу feed forward нейронных сетей или сетей прямого распространения. Выходной сигнал слоя в этих нейронных сетях передавался напрямую в следующий слой. Однако есть задачи, в которых нам нужно обучать нейронную сеть не на единичных экземплярах наподобие изображений, а на наборах последовательностей, например последовательностей слов. \n","\n","В рекуррентной нейронной сети выходной сигнал внутренних слоев циркулирует в этих слоях некоторое время. При обучении такой нейронной сети прежние выходные сигналы используются как дополнительные input'ы. Можно сказать, что эти дополнительные input'ы конкатенируются с \"нормальными\" input'ами предыдущего слоя.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zWU-4eQzEMdX"},"source":["![1.png](attachment:1.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6nGw0KEzEMdY"},"source":["Источник: https://medium.com/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y8TK-PtHEMdZ"},"source":["Реккурентные нейронные сети используются например для того, чтобы научить компьютерные системы \"понимать\" человеческих язык, для генерации текста. Также нейронные сети с подобной архитектурой могут использоваться для любых задач где осуществляется работа с некоторыми последовательностями значений, например с биржевыми котировками. Разновидности реккуретных нейронных используются также для постороения ИИ, подобных тем, что обыграли человека в компьютерную игру Dota 2. В отличие от сверточных нейронных сетей реккурентные нейронные сети как правило содержат небольшое количество слоев и например рекуррентая нейронная сеть в несколько десятков слоев будет считаться большой."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tRdYzz6SEMda"},"source":["## Архитектура Рекуррентных нейронных сетей\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M4H8swVREMdb"},"source":["Несмотря на то, что RNN могут хорошо справляться со своими задачами они не могут работать с длинными последовательностями. Эффективно они могут работать только с последовательностями состоящими из 3-4 элементов. Для, к примеру, анализа текста отзывов на предмет того положительный это отзыв или нет этого будет недостаточно. Здесь может понадобиться анализ нескольких десятков слов, чтобы сделать корректный вывод. Давайте обсудим почему обычной RNN не удается анализировать длинные последовательности. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R7KUC0zXEMdc"},"source":["#### Vanishing gradient problem\n","\n","Из материалов по сверточным нейронным сетям нам известна проблема исчезающего градиента. В случае с большим количеством слоев значение градиента при последовательном обновлении большого количества слоев становиться все меньше и может стать настолько маленьким, что не сможет в принципе существенно изменить поведение нейронов. В реккурентных нейронных сетях из-за сигнала циркулирующего внуртри слоев это проблема становиться еще острее. Причем градиент может стать не только очень маленьким, но и очень большим.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7FSlz4pbEMdd"},"source":["![2.png](attachment:2.png)![2.png](attachment:2.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iNhM9VwcEMde"},"source":["Источник: https://medium.com/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fNzFpeTAEMdf"},"source":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yzcXJUoOEMdg"},"source":["### Long Short Term Memory(LSTM)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OXGPBASIEMdh"},"source":["Решить проблему исчезающего градиента призвана разновидность RNN под названием LSTM.\n","\n","Long short-term memory (LSTM) юниты - это блоки из которых состоят слои одной из разновидностей рекуррентной нейронной сети(RNN). RNN состоящая из LSTM юнитов иногда называется просто LSTM. Обычно LSTM юнит представляет из себя ячейку состоящую из input gate, output gate и forget gate. Эти ячейки отвественны за запоминания значений на определенные промежутки времени.\n","\n","Каждый из этих элементов можно представить как типичный искусственный нейрон в многослойной неройнной сети, они вычисляют активацию(используя функцию активации) как взвешенную сумму. Их работа сводиться к регуляции потока значений через блок LSTM, поэтому они и называются ворота или затворы(gate). Понятие долгой памяти в названии возникло из-за того что они могут запоминать информацию на более длинный период времени чем обычная RNN. LSTM хорошо подходит для классификации процессов и предсказания временных последовательностей неизвестного размера и неизвестных промежутков между важными событиями. С технической точки зрения это достигается за счет ликвидации проблем связанных с exploding и vanishing gradient'ами."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q2n-UE5OEMdi"},"source":["![3.png](attachment:3.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZLoBNQCCEMdj"},"source":["Источник: https://medium.com/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TFxWGYsrEMdk"},"source":["### Компоненты LSTM.  \n","Ниже спискок приведен список компонентов из которых состоит ячейка LSTM:\n","\n","\n","<ol>\n","<li>Forget Gate \"f\" (нейронная сеть с сигмоидой)</li>\n","<li>Candidate layer \"С\" (нейронная сеть c Tanh)</li>\n","<li>Input Gate \"I\" (нейронная сеть с сигмоидой)</li>\n","<li>Output Gate \"O\"(нейронная сеть с сигмоидой)</li>\n","<li>Скрытоое состояние \"H\" (вектор)</li>\n","<li>Состояние памяти \"C\" (вектор)</li>\n","<li>Входы в LSTM ячейку на любом шаге Xt (текущий input) , Ht-1 (предыдущее скрытое сотояние ) и Ct-1 (предыдущее состояние памяти)</li>\n","<li>Выходы LSTM ячейки это Ht (текущее скрытое состояние ) и Ct (текущее состояние памяти)</li>\n","</ol>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VNjR4il0EMdk"},"source":["###### Работа затворов в LSTM\n","\n","\n","Во-первых LSTM ячейка берет предыдущее состояние памяти Ct-1 и умножает на значение в forget gate(f), чтобы определить, присутствует ли состояние памяти Ct. Если forget gate значение равно 0 то предыдущее состояние память полностью забывается, если же f forget gate значение равно 1 то предыдующее значение состояния памяти полностью проходит через ячейку(помните, что f gate дает значение между 0 и 1).\n","\n","Ct = Ct-1 * ft\n","\n","Вычисляем новое состояние памяти:\n","\n","Ct = Ct + (It * C`t)\n","\n","Теперь, вычисляем выходное значение:\n","\n","Ht = tanh(Ct)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CODShbUkEMdl"},"source":["### GRU\n","\n","Теперь, когда мы знаем как работает LSTM, давайте бегло взглянем на то, как работате GRU. GRU это более новое поколение рекуррентных нейронных сетейи и оно во многом похоже на LSTM. Но есть определенная разница. В GRU не используется состояние яченийки и используется скрытое состояние для передачи информации. В GRU также есть два затвора - reset gate и update gate."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y_i69-XXEMdm"},"source":["![4.png](attachment:4.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BmCaJM_WEMdn"},"source":["Источник: https://medium.com/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_cjME0yxEMdo"},"source":["Update Gate обновляет затворы действуя подобно forget и input gate, которые используются в LSTM. Он решает какая информация будет отброшена, а какая новая информация будет добавлена. Reset Gate это другой затвор использующийся для принятия решения как много прошлой информации будет забыто. В этих особенностях и заключается архитектура GRU. GRU имеет меньше тензорных операций и соответсвенно тренеруется быстре чем LSTM. Однако нельзя сказать точно какая архитектура лучше. Исследователи и инженеры пытаются определить, что в каждом конкретном случае подойдет больше. Если говорить упрощенно то GRU может подойти тогда когда важнее скорость чем точность, а LSTM тогда когда важнее точность чем скорость. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DVStY1AZEMdp"},"source":["## Практика"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BFELy5SWEMdq"},"source":["Давайте попробуем сделать простую реккурентную нейронную сеть, которая будет учиться складывать числа. Для этих целей мы не будем пользоваться фреймворками для Deep Learning, чтобы посмотреть как она работает внутри.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"98MqszYmEMdr","outputId":"44705ac8-df94-4f62-fe9d-de2f7edbc766"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error:[3.45638663]\n","Pred:[0 0 0 0 0 0 0 1]\n","True:[0 1 0 0 0 1 0 1]\n","9 + 60 = 1\n","------------\n","Error:[3.63389116]\n","Pred:[1 1 1 1 1 1 1 1]\n","True:[0 0 1 1 1 1 1 1]\n","28 + 35 = 255\n","------------\n","Error:[3.91366595]\n","Pred:[0 1 0 0 1 0 0 0]\n","True:[1 0 1 0 0 0 0 0]\n","116 + 44 = 72\n","------------\n","Error:[3.72191702]\n","Pred:[1 1 0 1 1 1 1 1]\n","True:[0 1 0 0 1 1 0 1]\n","4 + 73 = 223\n","------------\n","Error:[3.5852713]\n","Pred:[0 0 0 0 1 0 0 0]\n","True:[0 1 0 1 0 0 1 0]\n","71 + 11 = 8\n","------------\n","Error:[2.53352328]\n","Pred:[1 0 1 0 0 0 1 0]\n","True:[1 1 0 0 0 0 1 0]\n","81 + 113 = 162\n","------------\n","Error:[0.57691441]\n","Pred:[0 1 0 1 0 0 0 1]\n","True:[0 1 0 1 0 0 0 1]\n","81 + 0 = 81\n","------------\n","Error:[1.42589952]\n","Pred:[1 0 0 0 0 0 0 1]\n","True:[1 0 0 0 0 0 0 1]\n","4 + 125 = 129\n","------------\n","Error:[0.47477457]\n","Pred:[0 0 1 1 1 0 0 0]\n","True:[0 0 1 1 1 0 0 0]\n","39 + 17 = 56\n","------------\n","Error:[0.21595037]\n","Pred:[0 0 0 0 1 1 1 0]\n","True:[0 0 0 0 1 1 1 0]\n","11 + 3 = 14\n","------------\n"]}],"source":["# впервую очередь подключим numpy и библиотеку copy, которая понадобиться, чтобы сделать deepcopy ряда элементов\n","\n","import copy, numpy as np\n","np.random.seed(0)\n","\n","# вычислим сигмоиду\n","def sigmoid(x):\n","    output = 1/(1+np.exp(-x))\n","    return output\n","\n","# конвертируем значение функции сигмоиды в ее производную. \n","def sigmoid_output_to_derivative(output):\n","    return output*(1-output)\n","\n","# генерация тренировочного датасета\n","int2binary = {}\n","binary_dim = 8\n","\n","largest_number = pow(2,binary_dim)\n","binary = np.unpackbits(\n","    np.array([list(range(largest_number))],dtype=np.uint8).T,axis=1)\n","for i in range(largest_number):\n","    int2binary[i] = binary[i]\n","\n","# входные переменные\n","alpha = 0.1\n","input_dim = 2\n","hidden_dim = 16\n","output_dim = 1\n","\n","\n","# инициализация весов нейронной сети\n","synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n","synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n","synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n","\n","synapse_0_update = np.zeros_like(synapse_0)\n","synapse_1_update = np.zeros_like(synapse_1)\n","synapse_h_update = np.zeros_like(synapse_h)\n","\n","# тренировочная логика\n","for j in range(10000):\n","    \n","    # генерация простой проблемы сложения (a + b = c)\n","    a_int = np.random.randint(largest_number/2) # int version\n","    a = int2binary[a_int] # бинарное кодирование\n","\n","    b_int = np.random.randint(largest_number/2) # int version\n","    b = int2binary[b_int] # бинарное кодирование\n","\n","    # правильный ответ\n","    c_int = a_int + b_int\n","    c = int2binary[c_int]\n","    \n","    # место где мы располагаем наши лучше результаты (бинарно закодированные)\n","    d = np.zeros_like(c)\n","\n","    overallError = 0\n","    \n","    layer_2_deltas = list()\n","    layer_1_values = list()\n","    layer_1_values.append(np.zeros(hidden_dim))\n","    \n","    # движение вдоль позиций бинарной кодировки\n","    for position in range(binary_dim):\n","        \n","        # генерация input и output\n","        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n","        y = np.array([[c[binary_dim - position - 1]]]).T\n","\n","        # внутренний слой (input ~+ предыдущий внутренний)\n","        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n","\n","        # output layer (новое бинарное представление)\n","        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n","\n","        # проверка упустили ли мы что-то и если да, то как много \n","        layer_2_error = y - layer_2\n","        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n","        overallError += np.abs(layer_2_error[0])\n","    \n","        # декодируем оценку чтобы мы могли ее вывести на экран\n","        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n","        \n","        # сохраняем внутренний слой, чтобы мы могли его использовать в след. timestep\n","        layer_1_values.append(copy.deepcopy(layer_1))\n","    \n","    future_layer_1_delta = np.zeros(hidden_dim)\n","    \n","    for position in range(binary_dim):\n","        \n","        X = np.array([[a[position],b[position]]])\n","        layer_1 = layer_1_values[-position-1]\n","        prev_layer_1 = layer_1_values[-position-2]\n","        \n","        # величина ошибки в output layer\n","        layer_2_delta = layer_2_deltas[-position-1]\n","        # величина ошибки в hidden layer\n","        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n","\n","        # обновление всех весов и пробуем заново\n","        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n","        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n","        synapse_0_update += X.T.dot(layer_1_delta)\n","        \n","        future_layer_1_delta = layer_1_delta\n","    \n","\n","    synapse_0 += synapse_0_update * alpha\n","    synapse_1 += synapse_1_update * alpha\n","    synapse_h += synapse_h_update * alpha    \n","\n","    synapse_0_update *= 0\n","    synapse_1_update *= 0\n","    synapse_h_update *= 0\n","    \n","    # вывод на экран процесса обучения\n","    if(j % 1000 == 0):\n","        print(\"Error:\" + str(overallError))\n","        print(\"Pred:\" + str(d))\n","        print(\"True:\" + str(c))\n","        out = 0\n","        for index,x in enumerate(reversed(d)):\n","            out += x*pow(2,index)\n","        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n","        print(\"------------\")\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hcwdBFH6EMdy"},"source":["Теперь давайте попробуем с помощью Keras построить LSTM нейронную сеть для оценки настроений отзвывов на IMD."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2eu0D2NuEMdz"},"source":["Данный датасет слишком мал, чтобы преимущества LSTM проявились, однако в учебных целях он подойдет.\n","\n","В тренировке рекуррентных нейронных сетей важную роль играет размер batch, но еще большую роль играет выбор функций loss и optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"6d9bbL06EMd0","outputId":"f840da3e-f55f-4180-a43f-09a458fddf28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Загрузка данных...\n","25000 тренировочные последовательности\n","25000 тестовые последовательности\n","Pad последовательности (примеров в x единицу времени)\n","x_train shape: (25000, 80)\n","x_test shape: (25000, 80)\n","Построение модели...\n","Процесс обучения...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"]},{"name":"stdout","output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/1\n","25000/25000 [==============================] - 24s 966us/step - loss: 0.4608 - accuracy: 0.7844 - val_loss: 0.3825 - val_accuracy: 0.8336\n","25000/25000 [==============================] - 6s 232us/step\n","Результат при тестировании: 0.3825102120637894\n","Тестовая точность: 0.8335599899291992\n"]}],"source":["from __future__ import print_function\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import LSTM\n","from keras.datasets import imdb\n","\n","max_features = 20000\n","\n","# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n","maxlen = 80\n","batch_size = 50 # увеличьте значение для ускорения обучения\n","\n","print('Загрузка данных...')\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'тренировочные последовательности')\n","print(len(x_test), 'тестовые последовательности')\n","\n","print('Pad последовательности (примеров в x единицу времени)')\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n","\n","print('Построение модели...')\n","model = Sequential()\n","model.add(Embedding(max_features, 128))\n","model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров \n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","print('Процесс обучения...')\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=1, # увеличьте при необходимости\n","          validation_data=(x_test, y_test))\n","score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Результат при тестировании:', score)\n","print('Тестовая точность:', acc)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QTgLemMWEMd6"},"source":["Давайте также посмотрим пример в которм будет использоваться другой класс задач - генерация текста на основе тренировочного текста. В задачу нейросети будет входить обучившись на тексте Алиса в стране чудес и начать генерировать текст похожий на тот, что можно встретить в этой книге. Также в этом примере будет использоваться GRU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"qYuTLuwNEMd7","outputId":"a610b581-551c-4165-ced8-76a7d62eee8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","Итерация #: 0\n","Epoch 1/1\n","158773/158773 [==============================] - 14s 89us/step - loss: 2.3163\n","Генерация из посева: ry: ill tr\n","ry: ill treand the was the was the was the was the was the was the was the was the was the was the was the was\n"]}],"source":["import numpy as np\n","from keras.layers import Dense, Activation\n","from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n","from keras.models import Sequential\n","\n","\n","# построчное чтение из примера с текстом \n","with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n","    lines = []\n","    for line in _in:\n","        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n","        if len(line) == 0:\n","            continue\n","        lines.append(line)\n","text = \" \".join(lines)\n","chars = set([c for c in text])\n","nb_chars = len(chars)\n","\n","\n","# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n","# ID and a specific character. The numerical ID will correspond to a column\n","# ID и определенный символ. Numerical ID будет соответсвовать колонке\n","# число при использовании one-hot кодировки для представление входов символов\n","char2index = {c: i for i, c in enumerate(chars)}\n","index2char = {i: c for i, c in enumerate(chars)}\n","\n","# для удобства выберете фиксированную длину последовательность 10 символов \n","SEQLEN, STEP = 10, 1\n","input_chars, label_chars = [], []\n","\n","# конвертация data в серии разных SEQLEN-length субпоследовательностей\n","for i in range(0, len(text) - SEQLEN, STEP):\n","    input_chars.append(text[i: i + SEQLEN])\n","    label_chars.append(text[i + SEQLEN])\n","\n","\n","# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n","\n","X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n","y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n","for i, input_char in enumerate(input_chars):\n","    for j, ch in enumerate(input_char):\n","        X[i, j, char2index[ch]] = 1\n","    y[i, char2index[label_chars[i]]] = 1\n","\n","\n","# установка ряда метапамертров  для нейронной сети и процесса тренировки\n","BATCH_SIZE, HIDDEN_SIZE = 128, 128\n","NUM_ITERATIONS = 1 # 25 должно быть достаточно\n","NUM_EPOCHS_PER_ITERATION = 1\n","NUM_PREDS_PER_EPOCH = 100\n","\n","\n","# Create a super simple recurrent neural network. There is one recurrent\n","# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n","# encoded input layer. This is followed by a Dense fully-connected layer\n","# across the set of possible next characters, which is converted to a\n","# probability score via a standard softmax activation with a multi-class\n","# cross-entropy loss function linking the prediction to the one-hot\n","# encoding character label.\n","\n","'''\n","Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n","'''\n","\n","model = Sequential()\n","model.add(\n","    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n","        HIDDEN_SIZE,\n","        # возвращение последовательности True/False \n","        return_sequences=False,\n","        input_shape=(SEQLEN, nb_chars),\n","        unroll=True\n","    )\n",")\n","model.add(Dense(nb_chars))\n","model.add(Activation(\"softmax\"))\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n","\n","\n","# выполнение серий тренировочных и демонстрационных итераций \n","for iteration in range(NUM_ITERATIONS):\n","\n","    # для каждой итерации запуск передачи данных в модель \n","    print(\"=\" * 50)\n","    print(\"Итерация #: %d\" % (iteration))\n","    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n","\n","    # Select a random example input sequence.\n","    test_idx = np.random.randint(len(input_chars))\n","    test_chars = input_chars[test_idx]\n","\n","    # для числа шагов предсказаний использование текущей тренируемой модели \n","    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n","    print(\"Генерация из посева: %s\" % (test_chars))\n","    print(test_chars, end=\"\")\n","    for i in range(NUM_PREDS_PER_EPOCH):\n","\n","        # здесь one-hot encoding.\n","        X_test = np.zeros((1, SEQLEN, nb_chars))\n","        for j, ch in enumerate(test_chars):\n","            X_test[0, j, char2index[ch]] = 1\n","\n","        # осуществление предсказания с помощью текущей модели.\n","        pred = model.predict(X_test, verbose=0)[0]\n","        y_pred = index2char[np.argmax(pred)]\n","\n","        # вывод предсказания добавленного к тестовому примеру \n","        print(y_pred, end=\"\")\n","\n","        # инкрементация тестового примера содержащего предсказание\n","        test_chars = test_chars[1:] + y_pred\n","print()\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OcJKO5sKEMeA"},"source":["## Практическое задание\n","\n","<ol>\n","    <li>Попробуйте изменить параметры нейронной сети работающей с датасетом imdb либо нейронной сети работающей airline-passengers(она прилагается вместе с датасетом к уроку в виде отдельного скрипта) так, чтобы улучшить ее точность. Приложите анализ.</li>\n","    <li>Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.</li>\n","    <li>* Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM</li>\n","    <li>* Предложите свои варианты решения проблемы исчезающего градиента в RNN</li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dAzX21PbEMeB"},"source":["## Дополнительные материалы\n","\n","<ol>\n","    <li>Оригинальная научная статья по LSTM - https://www.bioinf.jku.at/publications/older/2604.pdf</li>\n","    <li>Оригинальная научная статья по GRU - https://arxiv.org/abs/1406.1078</li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LZ16qYvCEMeC"},"source":["## Используемая литература \n","\n","Для подготовки данного методического пособия были использованы следующие ресурсы:\n","<ol>\n","    <li>https://www.kaggle.com/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru</li>\n","    <li>Шакла Н. — Машинное обучение и TensorFlow 2019</li>\n","    <li>Николенко, Кадурин, Архангельская: Глубокое обучение. Погружение в мир нейронных сетей 2018</li>\n","    <li>Aurélien Géron - Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2019</li>\n","    <li>https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</li>\n","    <li>https://github.com/llSourcell/recurrent_neural_net_demo</li>\n","    \n","</ol>"]}],"metadata":{"colab":{"name":"metodich5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
