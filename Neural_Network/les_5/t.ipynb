{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгружаем модуль re для работы с текстом\n",
    "import re\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Загрузим текст\n",
    "with open('train_data.txt', 'r', encoding='utf-8') as f:\n",
    "    texts = f.read()\n",
    "    texts = texts.replace('\\ufeff', '') # убираем первый невидимый симво"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь нам нужно разбить эти высказывания на слова. Для этого воспользуемся уже знакомым из прошлого занятия инструментом Tokenizer и \n",
    "# положим, что максимальное число слов будет равно 1000:\n",
    "maxWordsCount = 1000\n",
    "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
    "                       lower=True, split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('вы', 3), ('лучший', 1), ('ответ', 1), ('на', 1), ('проблемы', 1), ('которые', 1), ('возникли', 1), ('в', 3), ('понедельник', 2), ('думайте', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Итак, мы разбили текст на слова и для примера выведем их начальный список:\n",
    "dist = list(tokenizer.word_counts.items())\n",
    "print(dist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее, мы преобразуем текст в последовательность чисел в соответствии с полученным словарем. \n",
    "# Для этого используется специальный метод класса Tokenizer:\n",
    "data = tokenizer.texts_to_sequences([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Осталось закодировать числа массива data в one-hot векторы. Для этого мы воспользуемся методом to_categorical пакета Keras:\n",
    "res = to_categorical(data[0], num_classes=maxWordsCount)\n",
    "print( res.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Затем, из этой матрицы сформируем тензор обучающей выборки и соответствующий набор выходных значений. \n",
    "# Для начала вычислим размер обучающего множества:\n",
    "inp_words = 3\n",
    "n = res.shape[0]-inp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И, далее, сформируем входной тензор и прогнозные значения также, как мы это делали с символами:\n",
    "X = np.array([res[i:i+inp_words, :] for i in range(n)])\n",
    "Y = res[inp_words:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 128)               144512    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              129000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,512\n",
      "Trainable params: 273,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Все, у нас есть обучающая выборка и требуемые выходные значения. Осталось создать модель рекуррентной сети. \n",
    "# Мы ее возьмем из предыдущего занятия с числом нейронов скрытого слоя 128 и maxWordsCount нейронами на выходе с функцией активации softmax:\n",
    "model_2 = Sequential()\n",
    "model_2.add(Input((inp_words, maxWordsCount)))\n",
    "model_2.add(SimpleRNN(128, activation='tanh'))\n",
    "model_2.add(Dense(maxWordsCount, activation='softmax'))\n",
    "model_2.summary()\n",
    " \n",
    "model_2.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем защиту от переобучения\n",
    "erly_stoping_callback = EarlyStopping(monitor='accuracy', patience=3)  # Если метрика 'val_accuracy' продолжает снижение два шага подряд,\n",
    "                                                                           # то выходим из процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.9048 - accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.8787 - accuracy: 0.0357\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8523 - accuracy: 0.0714\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.8253 - accuracy: 0.1786\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7975 - accuracy: 0.3571\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7685 - accuracy: 0.7143\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7382 - accuracy: 0.8929\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7062 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.6721 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.6356 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.5961 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Готово. Запускаем процесс обучения:\n",
    "history_2 = model_2.fit(X, Y, batch_size=32, epochs=20, callbacks=[erly_stoping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение остановлено на эпохе 10\n"
     ]
    }
   ],
   "source": [
    "# Смотрим на какой эпохе остановилось обучение\n",
    "if erly_stoping_callback.stopped_epoch >=1:\n",
    "    print('Обучение остановлено на эпохе', erly_stoping_callback.stopped_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И давайте теперь посмотрим, что у нас получилось. Запишем функцию для формирования текста из спрогнозированных слов:\n",
    "def buildPhrase(texts, str_len = 20):\n",
    "  res = texts\n",
    "  data = tokenizer.texts_to_sequences([texts])[0]\n",
    "\n",
    "  for i in range(str_len):\n",
    "    x = to_categorical(data[i: i+inp_words], num_classes=maxWordsCount) # преобразуем в One-Hot-encoding\n",
    "    inp = x.reshape(1, inp_words, maxWordsCount)\n",
    "    pred = model_2.predict( inp ) # предсказываем OHE четвертого символа\n",
    "    indx = pred.argmax(axis=1)[0]\n",
    "    data.append(indx)\n",
    " \n",
    "    res += \" \" + tokenizer.index_word[indx] # дописываем строку\n",
    " \n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# И вызовем ее с тремя первыми словами\n",
    "res = buildPhrase('которые возникли в')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "которые возникли в понедельник думайте позитивно и верьте в свою способность достигать отличных результатов если вы смогли в понедельник подняться с постели значит\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
